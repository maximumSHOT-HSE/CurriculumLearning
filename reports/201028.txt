[01:24]

Нашел прикольный алгоритм:
    * Условие - есть большая строка T, есть набор строк s_1, ..., s_n
        нужно для каждой s_i найти dist(T, s_i) - расстояние Левенштейна
    * Решение:
        Сначала рассмотрим наивное решение, а именно посчитаем тупую дпшку dp[i][j] - dist(s[0..i], T[0..j])
        Теперь заметим, что если мы добавим в s один символ справа, то матричка не сильно изменится, а именно к ней
        добавится новая строчка. А если мы удали один символ справа, то удалится одна строчка из матрички

        Тогда давайте положим все строчки в бор, будем его обходить, пересчитывая дпшку,
        тогда суммарное время работы = (размер бора) * |T|.

        Т.е. если у нас есть много строк с одинаковым общим префиксом, то можно намного быстрее посчитать
        расстояния

    Возникает интересный вопрос: пусть у нас есть набор из n строк и алфавит размера k. Строки непустые, длина каждой из строк <= L
    Насколько большой бор можно построить?
        Ответ:
            L * n + k * n / (k - 1) - log_k(n) * n

        Т.е. в общем случае сильно лучше не станет, но можно взять датасет, построить бор и посмотреть на его размер
* Нашел статью про "Fuzzy string matching"
    https://medium.com/@julientregoat/an-introduction-to-fuzzy-string-matching-178805cca2ab

    Основная идея - мы хотим оценить, насколько одна строка похожа на другую

    Говорят, что есть такие метрики, как "Sellers, Damerau-Levenshtein, Hamming"

[01:37]
    Очевидные рсстояние, которые приходят в голову
    * НОП
    * Расстояние Левешнтейна
    * Расстояние Дамерау-Левенштейна
    * Расстояние Хемминга (строки должны быть одинаковой длины)

[01:43] Нашел репозиторий с реализацией поиска всех строк на расстоянии Левенштейна <= d
    https://github.com/universal-automata/liblevenshtein/wiki

    В нем ссылаются на "Levenstein transducer"

[01:46] Есть еще какой-то "Locality-sensitive hashing"
    https://en.wikipedia.org/wiki/Locality-sensitive_hashing

[01:49] Ищу что-нибудь на тему "Semantic similarity"
    * http://www.nlp.town/blog/sentence-similarity/#:~:text=The%20easiest%20way%20of%20estimating,cosine%20between%20the%20resulting%20embeddings.
        # считаем косинусное расстояние (как в IR) - т.е. можно сказать, что есть запрос, хотим найти сколько-то ответов 
            (остальные строки - документы, по которым делаем поиск)
        # Word Mover's Distance
        # Smooth Inverse Frequency
        # Pre-trained encoders (InferSent, Google Sentence Encoder)

[01:58] Оставлю название супер книжки тут
    Chapelle, Sch¨olkopf, and Zien, 2006
    http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf

[01:59] 
    Нашел статью 2006 года про Semantic Similarity
        https://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf

        * Придумали метод оценки того, насколько два текста СЕМАНТИЧЕСКИ похожи
        * Говорят, что baseline - это посмотреть на лексическео отличие (всякие расстояния типа Левенштейна)
        * Говорят, что есть corpus-based (Turney 2001) и knowledge-base (Wu & Palmer 1994; Leacock & Chodorow 1998) подходы

        Из простых методов можно использовать лексическую близость + стемминг + удалание стоп слов + разметка частей речи + НОП + веса + нормализация

        ВАЖНО: авторы оценивают семантическую близость, используя только слова, причем без учета структуры предложений.
            Формула (1)
            Далее авторы рассматривают разные функции похожести для слов

            ВОПРОС: как можно учитывать структуру предложения?

        * Используют IDF для взвешивания слов (более редкие слова будут иметь больший вес)
        * Умеют сравнивать только слова из одинакового "класса" (например, только слова одинаковой части речи)
            Для слов из разных классов используют лексическую похожесть

        * Исследовали 2 corpus-based подхода и 6 knowledge-based подхода

        CORPUS-BASED
            (1) поточечная взаимная информация (PMI-IR, Turney, 2001)
                Что такое hits(x)?
            (2) latent semantic analysis
                Что они в итоге делают после SVD?
                    (похоже на PCA + косинусное расстояние)

        KNOWLEDGE-BASED
            Используют WordNet

            Вообще эти метрики сравнивают не слова, а концепты (значения этих слов) =>
            Давайте скажем, что f(w1, w2) = max_c1,c2 sim(c1, c2), где c1 - значение w, c2 - значение w2 (т.е. возьмем максимум по всем семантическим расстояниям
            значений этих слов)

        Есть проблемы с not (кажется, на нашу задачу это никак не влияет)
        В конце авторы говорят, что будут бороться с тем, что не учитывается структура предложения и взаимосвязи между словами
        Говорят про "semantic parse trees"

    И еще одну 1997 года
        https://arxiv.org/pdf/cmp-lg/9709008.pdf

[15:06] Читаю книжку
    Ыыыыы, в книжке говорят про dimensionality reduction

[15:39] Возникла мысль о том, что наша задача тесно связана с информационным поиском,
    т.к. мы хотим разбить датасет на группы семантически близких семплов.

[22:58]
    Вопрос: как лучше тренировать BERT?
        * Сгруппировать по лексической близости
        * Сгруппировать по семантической близости
        * Вообще не группировать, а стараться подсовывать разные предложения

[00:03]
    Возникла мысль, что нужно научиться оценивать сложность структуры предложения и учитывать ее тоже

[00:55] Нашел еще одну статью 2018 года на медиуме
    https://medium.com/@adriensieg/text-similarities-da019229c894
