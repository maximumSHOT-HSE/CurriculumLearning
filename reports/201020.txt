[20:40]

В прошлый раз мы обсуждали статью https://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf

Выделили несколько проблем и вопросов, с которыми нужно разобраться:

1) Для обучения BERT'а мы используем unsupervised learning, где нет никаких лейблов.
    Как адаптировать метод под нашу ситуацию?
    Откуда взять лейблы?
2) В статье использовали автокодироващики для получения эмбеддингов для картинок,
    но мы работаем с текстом, причем иногда с довольно большими предложениями =>
    эмбеддинг так просто не получить => хочется придумать альтернативу для эмбеддингов,
    либо адаптировать метод
3) Нужно понять, как реально с.ч. у Лапласиана влияют на то, как мы можем потенциально
    разделить наш датасет на куски (dataset complexity)

Возникла идея для алгоритма разделения датасет на группы:
    * Итеративно наращиваем группы
    * смотрим на текст, пытаемся найти по расстоянию Левенштейна самый близкий кусок, втсавляем туда
    * Тогда лейбл - это номер куска

    Нужно понять, как это быстро делать, т.е. возникла задача, есть много строк,
    они в каком-то порядке добавляеются, нужно искать ближайший кластер по расстоянию Левенштейна.
        Была идея про центроид (предложение, которое имеет минимальное суммарное расстояние до всех остальных строк в кластере)

        Непонятно, как быстро пересчитывать кластер?

[20:46] Пошел читать большую статью про спектральную кластеризацию (про с.ч.)
    https://arxiv.org/abs/0711.0189

[21:33] Прочитал треть - там несложная теория + простой алгоритм кластеризации датасета
    1) Строим similarity graph
    2) Получаем Лапласиан
    3) берем первые k с.в.
    4) Получаем матрицу n x k
    5) Заметим, что можно смотреть на нее, как на n векторов размерности k
    6) кластеризуем эти вектора на k кластеров с помощью k-means

    + есть несколько способов строить similarity graph (имея similarity fcuntion s(x, y))
        1) полный граф с весами s(xi, xj)
        2) (xi, xj) : (xi входит в k-ближайших к xj по s) и/или (xj входит в k-ближайших к xi по s)
            Т.е. для строк нужно быстро находить k ближайших по расстоянию Левенштейна, получим разреженный граф, у которого,
            возможно, получится найти k ближайших быстро
    + есть несколько видов Лапласианов (БУДЬТЕ ОСТОРОЖНЫ с L_{sym})

    Но все еще непонятно, как оценивать сложность какждого кластера, ведь меток у нас нет(

201021

[15:20] Продолжаю читать
    * Алгоритм кластеризации получен следующим образом
        1) Запишем в дискретном виде
        2) Заметим какие-то свойства
        3) Переходим в вещ. случай (забиваем на дискретность)
        3) Используем теорему Рэлея-Ритца (1993), которое дает решение в явном виде (k минимальных с.в.)
        4) Переходим обратно в дискретный случай с помощью k-means
    * Есть классная интерпретация в терминах случайного блуждания по графу:
        мы хотим разбить граф на части так, чтобы если мы будем случайно блуждать
        достаточно долго, то будем с высокой вероятностью оставаться в том же
        кластере, где и начали + будем с маленькой вероятностью переходить в другой кластер
    * Есть также объяснение тому, почему мы можем перейти в дискретный случай с помощью k-means
        теория возмущений говорит, что итоговые вектора будут близки к индикаторным (разница будет |H|/eigengap, где H - матрица возмущений)
        Этот результат называется теоремой Дэвиса-Кахана (1990-ые)
        В частности, из этой теоремы следует, что чем больше eigengap, тем меньше разница между дискретным случаем и аппрокисмированным случаем

=============================================================================================
[16:27]
* Для сравнения датасетов можно смотреть на их графики с.ч. в порядке возрастания глазами
    (в дополнение к числовой метрике CSG)
* Заметим, что у нас нет никаких эмбеддингов, есть только расстояние между строками, но строк может быть много
    # можно работать с несколькими кусками небольшого размера
* Опять же, чтобы не опираться на лейблы, можно смотреть на граф на самих строках,
    а потом смотреть на графики с.ч.


[16:37]
Как быстро искать с.ч.?? Подпространство Крылова наше все (Lanczos method (Golub and Van Loan, 1996))

Еще мысль по поводу меток и unsupervised learning. Кластеризация как раз и есть unsupervised learning.

[16:41]
Как определить число кластеров

* Fraley and Raftery (2002)
* (Still and Bialek, 2004)
* the gap statistic (Tibshirani, Walther, and Hastie, 2001)
* stability approaches (Ben-Hur, Elisseeff, and Guyon, 2002; Lange, Roth, Braun, and Buhmann, 2004; Ben-David, von Luxburg, and P´al, 2006)

В общем, можно взять threshold, тогда число кластеров = число с.ч. < threshold

[21:54]
* В статье сказано много аргументов в пользу L_{rw}

* Оооо, есть ссылки на использовние spectral clustering для
    semi-supervised g (e.g., Chapelle, Sch¨olkopf, and Zien, 2006 for an overview) 
    и для manifold reconstruction (e.g., Belkin and Niyogi, 2003)

========================
Вопросы?
* Вот мы поделили на кластеры, а как нужно учить наш BERT?
    # кормить по кластеру
    # брать кусками из каждого кластера
* Какой вид будут иметь кластеры?
* transductive classification problem - что это?
