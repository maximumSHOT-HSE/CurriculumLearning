Пытаюсь разобраться, как работать с датасетом викиепедии

Нашел крутой ноутбук: 
https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb#scrollTo=CHkz0ZHA0-Lz

* Вот так можно сохранять датасет
    https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter
* Вот так можно сохранять/загружать свой датасет
    https://github.com/tensorflow/tensorflow/issues/38483#issuecomment-614942557 - не работает
    https://github.com/tensorflow/tensorflow/issues/38483#issuecomment-638655441 - вроде работает

План:

[DONE] 0) подключиться к кластеру
[DONE] 0.5) научиться запускать код на кластере
0.9) Зафиксировать дамп википедии, на которой будем работать
0.95) В качестве ключа будем хранить md5 от текста
1) обучить SentencePieceTokenizer и сохранить его
2) 
    * собрать статистику по всему датасету википедии и сохранить (сериализовать)
        a) cnt[(position, token)]
        b) cnt[(position, previous_token, current_token)]
    * собрать статистику по 1000 текстам (для тестов) и сохранить
3) написать код для ExcessEntropy и протестировать его
    Простой тест
        а) взять статистику по 1000 текстам
        b) Придумать 10 коротких предложений -> протокенизировать
        с) Руками посчитать на них ExcessEntropy
    Стресс тест
        а) взять статистику по 1000 текстам
        b) взять 500 текстов из википедии (и зафиксировать)
        с) написть код за квадрат
        d) запустить код за квадрат и за линию, сравнить ответы
4) Посчитать Excess
