[18:43] Выписываю фичи из статьи https://arxiv.org/ftp/arxiv/papers/2001/2001.01863.pdf

    Фонологические признаки
        Считаем просто количество

        Слоги - CMU словарь (есть в NLTK toolbox)
            CMU словарь имеет размер 133737
            но этого не достаточно => используют специальный аглоритм (https://www.howmanysyllables.com/howtocountsyllables)
        Фонемы
        Графемы (символы)

    Морфологические признаки
        Морфологическое разнообразие
            Лемма - это простая форма слова, для того, чтобы ее получить, используют WordNet (есть в NLTK)
            Morphological diversity = #{different lemmas} / # {total number of lemmas}

        Сложность слова
            Среднее число суффиксов на слово с большим "эффектом" (что такое эффект?)

        Среднее длина существительных, прилагательных, глаголов и наречий

        Форма глагола

    Лексические признаки
        Лексическая плотность (LEXdens) = (число слов, несущих информацию) / (общее число слов) = #{lexical words} / #{total number of words}
        Как определить, что слово несет информацию?
            Это делают по-разному
                [O’Loughlin, 1995] - все наречия времени, поведения и места
                 [Engber, 1995] and [Lu, 2012] - существительные, прилагательные, глаголы (без модальных глаголов, искусственных слов как be или have), наречия, основанные на прилагательных (добавили -ly)
        Лексическая утонченность/редкость (LEXsph) = #{sophisticated words} / #{lexical words}
            Слово называется sophisticated, если оно редкое (можно взять частоту из WFD(Word Frequency Data))
            + добавили Porter Stemmer

            CLS (Continuous Lexical Sophistication)
            VSM (Verb Sophistication Measure)
            VSM Sq.
        Число различных слов
        Type Token Ratio
        MTLD (The Measure of Textual Lexical Diversity)
        HD-D

    Синтаксические признаки (кажется, наиболее подходящие признаки касательно нашей задачи)
        Парсим с помощью Stanford Parser
        Используем NLTK (например, для токенизации и для разметки частей речи)

        Фразовый уровень
            вытаскиваем фразы с помощью Stanford Parser'а

            RatioNP = #NP / #XP (здесь, #NP - кол-во noun-phrases, а #XP - общее число фраз)
            Группа фичей - (число фраз в тексте) / (число предложений в тексте)

        T-юниты
            T-юнит - это грамматически кратчайшее предложение, обычно он состоит из одной главной части и нескольких придаточных
            Можно придумать inf много признаков

        N-граммы

    Логические признаки
        Целостность текста
            Давайте посмотрим на соединительные слова (hence, because, but, thus, etc) и посчитаем кучу разных статистик
        Алгоритм Хоббса [Hobbs, 1978]

    Психолингвистические признаки
        Существует куча разных признаков, которые можно достать, используя базу данных MRC

    Читабельность текста
        Gunning’s Fog index
        The Flesch-Kincaid formula
        Coleman-Liau Index
        Spache readability formula
        The Dale–Chall formula
        Automated Readability Index
        The FORCAST Readability Formula

    Вообще, в статье представлено очень много разных признаков, но по отдельности они не так сильны
    Авторы для своих экспериментов используют датасеты
        ESLTL
        BAWE
        CTT
        NOW

    Можно воспроизвести их эксперимент (или сделать что-то похожее), используя какой-нибудь классификатор
    Т.е. нужно обучить классификатор определять сложность датасета (который они предоставили), дальше
    с помощью этого классификатора разметить неразмеченные данные и скормить BERT'у

[19:51] Внезапная идея: может сложность датасета как-то связана с тем, насколько хорошо ее можно сжать?
    Нужно поискать статьи на тему сжатия данных

[20:49]
    Нашел несколько статей про data compression

    https://d1wqtxts1xzle7.cloudfront.net/61774586/Comp_com20200113-81589-6qysrz.pdf?1578973394=&response-content-disposition=inline%3B+filename%3DCOMPARISON_OF_LOSSLESS_DATA_COMPRESSION.pdf&Expires=1604515025&Signature=ACQ3s1PiMUPh-WcZPD4juEfuNrEaTEVeu0uE93ikv~31xd0DxZHTGFR1waQzFBsCQ5kg-bT7XcwWFE5eVDwfHnzJad6r2n9pcnXlo~oj3HJNxThrVx6Ylq-Xf9PcWSbO-xqBpa2E8Me~pHMbTKEGpNMfc94wb89oi63mFYRRi-kHjQNuKNOPcRBhLH3s5iAjrbesUR6OC5q0VewICQ5vMJ2ExEyeD~ER4ZZ-i81ETTMrciA0dLZO1~mdXVudWLA1SAe5TTbiW8QKnnyQM4v93bNs3b0LzRLrGrHwOlAlfVGxELGGRNevaU7kmMC~YVA2k--497QnZfivqGuuVXK2gg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA

    https://www.researchgate.net/publication/322387005_A_SURVEY_ON_LOSSLESS_TEXT_DATA_COMPRESSION_TECHNIQUES/link/5ae84658aca2725dabb399ec/download

[13:18] Нашел статью про Representation Learning в NLP
    https://www.researchgate.net/publication/342680129_Representation_Learning_and_NLP/link/5f007f50299bf18816005e45/download

    Хм, оказывается, это какая-то глава из какой-то книги

[13:38] Идея для еще одной метрики - это среднее расстояние между словами в двух предложениях в семантическом пространстве
    Т.е. мы берем все пары слов в двух предложениях, переводим слова в эмбеддинги (word2vec, GloVe, fastText), считаем расстояние (Евклидово) и считаем
    среднее по таким расстояниям.

[13:49] Еще идея - попробовать использовать простые LM-ки

[13:51] Мысль по поводу кластеризации - на данный момент мы нашли два вида признаков - pointwise и pairwise
    => Можно использовать два вида кластеризации
        Для pointwise используем обычную кластеризацию (text -> features -> k-means), таким образом получаем лейблы
        pairwise фичи используем как расстояния для того, чтобы можно было использовать spectral clustering

[13:55] Статья про curriculum learning
    https://www.researchgate.net/publication/221344862_Curriculum_learning/link/546cd2570cf2193b94c577ac/download

    Есть какие-то кукареки про то, что curriculum learning полезен
