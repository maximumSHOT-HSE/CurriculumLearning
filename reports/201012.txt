

[14:00] Прочитал статью https://arxiv.org/pdf/2004.03720.pdf
    Аторы показали, что unigram LM Tokenization лучше Byte Pair Encoding tokenization

TODO: прочитать про 

* WordPiece (https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/37842.pdf)
* https://arxiv.org/pdf/2004.03844.pdf
* https://arxiv.org/pdf/2001.04451.pdf

[14:30] Ищу статьи по теме "Dataset Complexity"

* 2006, Data Complexity in Machine Learning (https://authors.library.caltech.edu/27081/1/dcomplex.pdf)
* 2019, Spectral Metric for Dataset Complexity Assessment (https://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf)
    Почти то, что нужно, но для изображений
* Dataset Complexity and Gene Expression Based Cancer Classification (https://link.springer.com/chapter/10.1007/978-3-540-73400-0_61)

[14:40] Идея: кластеризовать датасет (может получится выделить кластеры по сложности)

* 2015, Short Text Clustering via Convolutional Neural Networks (https://www.aclweb.org/anthology/W15-1509.pdf)

[14:46] идея для алгоритма обучения BERTA
    1. Учим модель
    2. Оцениваем таски (например, SquAD)
    3. Получили компаратор => сортируем
    4. Заново обучаем модель, используя новый порядок
    5. Продолжаем пока не надоест

    +: хоть какой-то алгоритм) (надо экспериментировать)
    -: очень, очень долго...; зависимость следующих поколений от старых (аля генетический алгоритм)

[14:52] Можно использовать идеи из IR
    * может там есть какие-нибудь метрики-ранги, которые умеют оценивать сложность, актуальность и т.п. характеристики документа

[14:56] Кажется, нашел релевентную статью

    Classification and clustering for neuroinformatics: Assessing the efficacy on reverse-mapped NeuroNLP data using standard ML techniques (https://www.researchgate.net/publication/285614445_Classification_and_clustering_for_neuroinformatics_Assessing_the_efficacy_on_reverse-mapped_NeuroNLP_data_using_standard_ML_techniques)

[14:59] Идея - решить задачу классификации текстов на группы: для какого возраста релевантен данный текст
    Сложности: где взять данные?
    Решения:
        * Собрать датасет, основанный на учебниках в школах и университетах (мы знаем текст и номер курса => можем классифицировать), в частности, кажется, что учебники по изучению языков будут наиболее подходящими + есть деление на basic, intermediate, advanced, etc.
        * Взять какой-нибудь открытый источник текстов, где люди могут комментировать, и где мы можем узнать возраст человека
        * Обучить модель классифицировать тексты
        * Использовать обученную модель для классификации уже нужных нам датасетов, после классификации можем поделить датасет на части и учить нашими алгоритмами

[15:04] Нашел статью про регионализацию (https://www.researchgate.net/publication/334973916_A_regionalization_method_for_clustering_and_partitioning_based_on_trajectories_from_NLP_perspective)

    Насколько я понял, авторы кластеризуют датасет на части по региону (видимо, содержание должно относиться к конкретному региону)

    К ней нет доступа.

[15:08] Статья 2010-го года про "Using Complexity Measures in Information Retrieval"

    https://www.researchgate.net/publication/47936988_Using_Complexity_Measures_in_Information_Retrieval

    Бритва Оккама: "Если есть две теории, объясняющие одно явление, то лучше та, которая меньше".
    Люди оценивают сложность документа по "understandability [24], comprehensibility [18], and accessibility"
    (да, это понимание, понимание и еще понимание)

    Psychology
    - channel capacity
    - Hick’s law (more information leads to longer decision times)
    - A higher textual coherence leads to better comprehension.
    - content overlap
    - coherence on a grammar level and a semantic level

    Метрики оценки сложности текста (статистические фичи)
    - Flesch Reading Ease Scale:
        $S_f = 206.84 - 84.60 * SpW - 1.02 * Wps$, where
        $SpW$ -- кол-во слогов на слово (среднее число слогов в слове)
        $WpS$ -- кол-во слов на предложение (среднее число слов в предложении)

    - Flesch-Kincaid Readability formula
        $G_f$ -- reading grade level (какой уровень чтения нужен для того, чтобы прочитать этот текст)
        $G_f = 0.39 * WpS + 11.80 * SpW - 15.59$

    - Fog Index
        $E_f$ - кол-во лет обучения, необходимое для понимания текста
        $E_f = 0.40 * WpS + PW / W * 100$, where
        $W$ - кол-во слов в тексте
        $PW$ - кол-во многосложных слов

    - SMOG Readability Formula
        $G_s$ - минимальный reading grade для текста
        $G_s = 3 + sqrt(30 * PW / S)$, where
        $S$ - число предложений

    ergodic signal???

    Метрика оценки кол-ва информации в тексте - энтропия

    Семантическая сложность
        W = WordNet v3 - лексическая база данных

        в 2003 году Gervasi и Ambriola выпустили метод оценки кол-ва знаний, необходимых для понимания текста:
        они для каждого слова w текста считают кол-во концептов/понятий (concepts) которые есть за следующие n
        шагов чтения у этого слова. Чем больше смыслов у слова, тем проще текст (внезапно).

    Семантическая согласованность (отвечает за то, насколько несколько предложений связаны друг с другом)
        Общая согласованность текста оценивается как средняя похожесть двух соседних предложений (текст - это последовательность предложений)
        Как посчитать похожесть двух предложений? Давайте возьмем среднюю похожесть двух слов в этих двух предложениях.
        Как посчитать похожесть двух слов w1 и w2? Давайте найдем наибольшую похожесть по всем парам phi1, phi2, где phi - группа синонимов в нашей базе
        Как посчитать похожесть двух групп синонимов? Давайте найдем длину кратчайшего пути в WordNet (semantic netowrk) (St-Onge????)

    Датасеты:
    - Eglish Wikipedia
    - Simple English Wikipedia
    
    Как оценивать фичи и дажа сравнивать два датасета?
    - the point biserial correlation coefficient with a pooled sample standard deviation
    - Mann-Whitney U^N , a non-parametric coefficient of statistical power

    Считаем для одной фичи между двумя датасетами, после смотрим на число, авторы дали примерное описание значений

    Итого:
    * ввели кучу метрик для оценки разной сложности текстов
    * научились оценивать эти метрики, сравнивать два набора данных
    * есть simple wikipedia (уже можно тестировать наши идеи)
    * на самом деле, мы придумаи много фичей, можно их комбинировать и говорить, что это вектор фичей, после кластеризовать, дальше посортить руками (кластеров мало), пытаться обучать bert, помотреть глазами
    * много ссылок по той же теме
    * огонь статья

[15;16] Еще какая-то статья 2015-го года про "Collection complexity"

    https://link.springer.com/chapter/10.1007/978-3-319-24027-5_23

    Но она тоже закрытая(
