

[14:00] Прочитал статью https://arxiv.org/pdf/2004.03720.pdf
    Аторы показали, что unigram LM Tokenization лучше Byte Pair Encoding tokenization

TODO: прочитать про 

* WordPiece (https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/37842.pdf)
* https://arxiv.org/pdf/2004.03844.pdf
* https://arxiv.org/pdf/2001.04451.pdf

[14:30] Ищу статьи по теме "Dataset Complexity"

* 2006, Data Complexity in Machine Learning (https://authors.library.caltech.edu/27081/1/dcomplex.pdf)
* 2019, Spectral Metric for Dataset Complexity Assessment (https://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf)
    Почти то, что нужно, но для изображений (однако авторы говорят, что обобщается до других задач: например, NLP)

    2019 год

    cumulative spectral gradient (CSG) - метод оценки сложности решения задачи классификации изображений на данном датасете
    Эта метрика позволяет выяснить, какие классы сложнее всего выделяют CNN

    Метрика сильно коррелирует с test accuracy

    Как можно просто оценить сложность датасета? Давайте на нем обучим модель и посмотрим на test accuracy.
    Чтобы сравнить два датасета, нужно сравнить test accuracy одной и той же модели

    Но это долго + нужен полностью размеченный датасет

    Говорят про какую-то C-measure (complexity measure)

    * Решаем задачу классификации для изображений
    * Давайте для наших данных получим эбмеддинги (как-нибудь)
    * Сначала посчитаем "class overlap" (перекрытие классов) с помощью интеграла
        Интеграл показывает, насколько похожи распределения двух классов в пространстве фичей
        Но интеграл считать довольно сложно
    * Заметим, что можно считать похожую вещь, а именно какое-нибудь расстояние между распределениями
        Кульбак-Лейблер, тест Колмогорова-Смирнова,
        авторы же используют "kernel of Jebara" (формула (2)) - обобщение, например, расстояния Хеллингера
    * Для произвольного $\rho$ такую штуку все еще сложно считать, поэтому давайте скажем, что $\rho=1$ и заметим, что
        это аля скалаярное произведение двух распределений,
        также можно сказать, что это какое-то мат. ожидание.
        Давайте эту апрокисмируем эту штуку с помощью метода Монте Карло (формула (3))
        Посчитать $P(\phi(x_m)|C_j)$ в явном виде тоже не можем (сложно) => аппрокисмируем так:
        плотность будет такой $p(\phi(x_m)|C_j) = \frac{K_{C_j}}{M\cdot V}$, пусть есть $\phi(x)$, найдем $k$ ближайших к ней точек из класса С_j,
        возьмем минимальный гиперкуб, содержащий все эти $k$ соседей, скажем, что $V$ -- это объем этого гиперкуба. $K_{C_j}$ -- число соседей из класса
        $C_j$ (видимо, это $k$), а $M$ -- общее число семплов в классе $C_j$
    * Т.е. мы научились считать какое-то расстояние между двумя классами => построим матрицу $S$ размера KxK, где K - число классов
    * Теперь хотим достать какую-то меру из этой матрицы $S$, чтобы оценить итговую сложность датасета
    * И тут вступает в бой теория спектральной кластеризации. Краткий озбор:
        Давайте рассмотрим взвешенный граф G=(V, E) с весами $w_{i,j} \in [0, 1]$, будем говорить, что эти веса отвечают за "похожесть" двуз вершин.
        Если вес равен нулю, то $i, j$ не соединены ребром (вообще не похожи)
        Если вес равен единице, то $i, j$ одинаовые (абсолютно похожи)
        Давайте рассмотрим матрицу смежности W этого графа. Заметим, что она симметрична и положительно полуопределена (x^T * W * x >= 0) ???
        Цель - разделить наш граф на подграфы, чтобы величина разреза была минимальной (величина разреза - это сумма весов ребер между вершинами из разных подграфов).
        [Хм, почему нельзя просто сделать один большой подграф, тогда ведь вес будет нулем]

        Давайте сначала построим лапласиан L = D - W, где D - диагональная марица с $D_i = \sum_j w_{i,j}$,
        L - симметричная положительно полуопределенная матрица. [Почему?: можно в явном виде расписать и получить]
        Посмотрим на ее спектр (собственные числа): $\{\lambda_0, \ldots, \lambda_{n-1}\}$, все они вещественные и неотрицательные [Почему?: неотрицательные, потому что L - положительно полуопределена (x.T @ L @ x >= 0, если подставить с.в. x, то получим lambda * |x| >= 0); вещественные, потому что L - самосопряженный оператор, более того, существует ортонормированный базис из с.в. (спектральная теорема из алгебры)], причем минимальное равно нулю (потому что detL = 0, т.к. сумма в каждой строке равна нулю).
        Супер факт - собственные вектора определяют разделение графа на подграфы, а собственные числа определяют вес разреза [Как? Что? Почему?]
    * Скажем, что в нашем графе вершины - это классы, а ребра имеют вес, полученный с помощью формулы (5)
    * Итоговая метрика считает сумму префиксных максимумов в последовательности нормализованных eigen gap'ов

    Крутая статья

    Вопросы
    1. Что с эмбеддингами
    2. Зачем кумулятивные максимумы?
    3. Как применить к NLP? (Довольно сложно получить эмбединги для текстов)

* Dataset Complexity and Gene Expression Based Cancer Classification (https://link.springer.com/chapter/10.1007/978-3-540-73400-0_61)

[14:40] Идея: кластеризовать датасет (может получится выделить кластеры по сложности)

* 2015, Short Text Clustering via Convolutional Neural Networks (https://www.aclweb.org/anthology/W15-1509.pdf)

[14:46] идея для алгоритма обучения BERTA
    1. Учим модель
    2. Оцениваем таски (например, SquAD)
    3. Получили компаратор => сортируем
    4. Заново обучаем модель, используя новый порядок
    5. Продолжаем пока не надоест

    +: хоть какой-то алгоритм) (надо экспериментировать)
    -: очень, очень долго...; зависимость следующих поколений от старых (аля генетический алгоритм)

[14:52] Можно использовать идеи из IR
    * может там есть какие-нибудь метрики-ранги, которые умеют оценивать сложность, актуальность и т.п. характеристики документа

[14:56] Кажется, нашел релевентную статью

    Classification and clustering for neuroinformatics: Assessing the efficacy on reverse-mapped NeuroNLP data using standard ML techniques (https://www.researchgate.net/publication/285614445_Classification_and_clustering_for_neuroinformatics_Assessing_the_efficacy_on_reverse-mapped_NeuroNLP_data_using_standard_ML_techniques)

    Мотивация работы: NLP модели зависят от качества датасета по теме => нужно находить такие датасеты, говорят, что можно опираться на ключевые слова о датасете, но у этого подхода есть минусы (например, используются автоматические key words extractor'ы, что не очень хорошо) => пытаются кластеризовать документы (тексты)

    Используют предобработанные данные и какой-то "approach of reverse mapping mesh words from MetaMap"

    Не подходит(

[14:59] Идея - решить задачу классификации текстов на группы: для какого возраста релевантен данный текст
    Сложности: где взять данные?
    Решения:
        * Собрать датасет, основанный на учебниках в школах и университетах (мы знаем текст и номер курса => можем классифицировать), в частности, кажется, что учебники по изучению языков будут наиболее подходящими + есть деление на basic, intermediate, advanced, etc.
        * Взять какой-нибудь открытый источник текстов, где люди могут комментировать, и где мы можем узнать возраст человека
        * Обучить модель классифицировать тексты
        * Использовать обученную модель для классификации уже нужных нам датасетов, после классификации можем поделить датасет на части и учить нашими алгоритмами

[15:04] Нашел статью про регионализацию (https://www.researchgate.net/publication/334973916_A_regionalization_method_for_clustering_and_partitioning_based_on_trajectories_from_NLP_perspective)

    Насколько я понял, авторы кластеризуют датасет на части по региону (видимо, содержание должно относиться к конкретному региону)

    К ней нет доступа.

[15:08] Статья 2010-го года про "Using Complexity Measures in Information Retrieval"

    https://www.researchgate.net/publication/47936988_Using_Complexity_Measures_in_Information_Retrieval

    Бритва Оккама: "Если есть две теории, объясняющие одно явление, то лучше та, которая меньше".
    Люди оценивают сложность документа по "understandability [24], comprehensibility [18], and accessibility"
    (да, это понимание, понимание и еще понимание)

    Psychology
    - channel capacity
    - Hick’s law (more information leads to longer decision times)
    - A higher textual coherence leads to better comprehension.
    - content overlap
    - coherence on a grammar level and a semantic level

    Метрики оценки сложности текста (статистические фичи)
    - Flesch Reading Ease Scale:
        $S_f = 206.84 - 84.60 * SpW - 1.02 * Wps$, where
        $SpW$ -- кол-во слогов на слово (среднее число слогов в слове)
        $WpS$ -- кол-во слов на предложение (среднее число слов в предложении)

    - Flesch-Kincaid Readability formula
        $G_f$ -- reading grade level (какой уровень чтения нужен для того, чтобы прочитать этот текст)
        $G_f = 0.39 * WpS + 11.80 * SpW - 15.59$

    - Fog Index
        $E_f$ - кол-во лет обучения, необходимое для понимания текста
        $E_f = 0.40 * WpS + PW / W * 100$, where
        $W$ - кол-во слов в тексте
        $PW$ - кол-во многосложных слов

    - SMOG Readability Formula
        $G_s$ - минимальный reading grade для текста
        $G_s = 3 + sqrt(30 * PW / S)$, where
        $S$ - число предложений

    ergodic signal???

    Метрика оценки кол-ва информации в тексте - энтропия

    Семантическая сложность
        W = WordNet v3 - лексическая база данных

        в 2003 году Gervasi и Ambriola выпустили метод оценки кол-ва знаний, необходимых для понимания текста:
        они для каждого слова w текста считают кол-во концептов/понятий (concepts) которые есть за следующие n
        шагов чтения у этого слова. Чем больше смыслов у слова, тем проще текст (внезапно).

    Семантическая согласованность (отвечает за то, насколько несколько предложений связаны друг с другом)
        Общая согласованность текста оценивается как средняя похожесть двух соседних предложений (текст - это последовательность предложений)
        Как посчитать похожесть двух предложений? Давайте возьмем среднюю похожесть двух слов в этих двух предложениях.
        Как посчитать похожесть двух слов w1 и w2? Давайте найдем наибольшую похожесть по всем парам phi1, phi2, где phi - группа синонимов в нашей базе
        Как посчитать похожесть двух групп синонимов? Давайте найдем длину кратчайшего пути в WordNet (semantic netowrk) (St-Onge????)

    Датасеты:
    - Eglish Wikipedia
    - Simple English Wikipedia
    
    Как оценивать фичи и даже сравнивать два датасета?
    - the point biserial correlation coefficient with a pooled sample standard deviation
    - Mann-Whitney U^N , a non-parametric coefficient of statistical power

    Считаем для одной фичи между двумя датасетами, после смотрим на число, авторы дали примерное описание значений

    Итого:
    * ввели кучу метрик для оценки разной сложности текстов
    * научились оценивать эти метрики, сравнивать два набора данных
    * есть simple wikipedia (уже можно тестировать наши идеи)
    * на самом деле, мы придумаи много фичей, можно их комбинировать и говорить, что это вектор фичей, после кластеризовать, дальше посортить руками (кластеров мало), пытаться обучать bert, помотреть глазами
    * много ссылок по той же теме
    * огонь статья

[15;16] Еще какая-то статья 2015-го года про "Collection complexity"

    https://link.springer.com/chapter/10.1007/978-3-319-24027-5_23

    Но она тоже закрытая(
