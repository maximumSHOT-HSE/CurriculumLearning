@inproceedings{hacohen2019power,
	title={On the power of curriculum learning in training deep networks},
	author={Hacohen, Guy and Weinshall, Daphna},
	booktitle={International Conference on Machine Learning},
	pages={2535--2544},
	year={2019},
	organization={PMLR}
}
@article{narvekar2020curriculum,
	title={Curriculum learning for reinforcement learning domains: A framework and survey},
	author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={181},
	pages={1--50},
	year={2020}
}
@article{mermer2017scalable,
	title={Scalable Curriculum Learning for Artificial Neural Networks},
	author={Mermer, Melike Nur and Amasyali, Mehmet Fatih},
	journal={IPSI BGD TRANSACTIONS ON INTERNET RESEARCH},
	volume={13},
	number={2},
	year={2017},
	publisher={IPSI BELGRADE LTD DALMATINSKA 55, BELGRADE, 00000, SERBIA}
}
@article{wu2020curricula,
	title={When Do Curricula Work?},
	author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
	journal={arXiv preprint arXiv:2012.03107},
	year={2020}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={arXiv preprint arXiv:1706.03762},
	year={2017}
}
@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={arXiv preprint arXiv:2005.14165},
	year={2020}
}
@inproceedings{gertner2019mitre,
	title={MITRE at SemEval-2019 task 5: Transfer learning for multilingual hate speech detection},
	author={Gertner, Abigail S and Henderson, John and Merkhofer, Elizabeth and Marsh, Amy and Wellner, Ben and Zarrella, Guido},
	booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
	pages={453--459},
	year={2019}
}
@inproceedings{bengio2009curriculum,
	title={Curriculum learning},
	author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
	booktitle={Proceedings of the 26th annual international conference on machine learning},
	pages={41--48},
	year={2009}
}
@article{platanios2019competence,
	title={Competence-based curriculum learning for neural machine translation},
	author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom M},
	journal={arXiv preprint arXiv:1903.09848},
	year={2019}
}
@inproceedings{xu2020curriculum,
	title={Curriculum learning for natural language understanding},
	author={Xu, Benfeng and Zhang, Licheng and Mao, Zhendong and Wang, Quan and Xie, Hongtao and Zhang, Yongdong},
	booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	pages={6095--6104},
	year={2020}
}
@article{kocmi2017curriculum,
	title={Curriculum learning and minibatch bucketing in neural machine translation},
	author={Kocmi, Tom and Bojar, Ondrej},
	journal={arXiv preprint arXiv:1707.09533},
	year={2017}
}
@article{zhang2018empirical,
	title={An empirical exploration of curriculum learning for neural machine translation},
	author={Zhang, Xuan and Kumar, Gaurav and Khayrallah, Huda and Murray, Kenton and Gwinnup, Jeremy and Martindale, Marianna J and McNamee, Paul and Duh, Kevin and Carpuat, Marine},
	journal={arXiv preprint arXiv:1811.00739},
	year={2018}
}
@article{kurdi2020text,
	title={Text Complexity Classification Based on Linguistic Information: Application to Intelligent Tutoring of ESL},
	author={Kurdi, M Zakaria},
	journal={arXiv preprint arXiv:2001.01863},
	year={2020}
}
@inproceedings{van2010using,
	title={Using complexity measures in information retrieval},
	author={van der Sluis, Frans and van den Broek, Egon L},
	booktitle={Proceedings of the third symposium on information interaction in context},
	pages={383--388},
	year={2010}
}
@inproceedings{ay2006unifying,
	title={A unifying framework for complexity measures of finite systems},
	author={Ay, Nihat and Olbrich, Eckehard and Bertschinger, Nils and Jost, J{\"u}rgen},
	booktitle={Proceedings of ECCS},
	volume={6},
	year={2006},
	organization={Citeseer}
}
@inproceedings{srivastava2020noisy,
	title={Noisy Text Data: Achillesâ€™ Heel of BERT},
	author={Srivastava, Ankit and Makhija, Piyush and Gupta, Anuj},
	booktitle={Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},
	pages={16--21},
	year={2020}
}
@article{lan2019albert,
	title={Albert: A lite bert for self-supervised learning of language representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	journal={arXiv preprint arXiv:1909.11942},
	year={2019}
}
@article{lewis2019bart,
	title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	journal={arXiv preprint arXiv:1910.13461},
	year={2019}
}
@article{he2020deberta,
	title={Deberta: Decoding-enhanced bert with disentangled attention},
	author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	journal={arXiv preprint arXiv:2006.03654},
	year={2020}
}
@article{sanh2019distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	journal={arXiv preprint arXiv:1910.01108},
	year={2019}
}
@article{le2019flaubert,
	title={Flaubert: Unsupervised language model pre-training for french},
	author={Le, Hang and Vial, Lo{\"\i}c and Frej, Jibril and Segonne, Vincent and Coavoux, Maximin and Lecouteux, Benjamin and Allauzen, Alexandre and Crabb{\'e}, Beno{\^\i}t and Besacier, Laurent and Schwab, Didier},
	journal={arXiv preprint arXiv:1912.05372},
	year={2019}
}
@article{nguyen2020phobert,
	title={PhoBERT: Pre-trained language models for Vietnamese},
	author={Nguyen, Dat Quoc and Nguyen, Anh Tuan},
	journal={arXiv preprint arXiv:2003.00744},
	year={2020}
}
@article{liu2019roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{iandola2020squeezebert,
	title={SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},
	author={Iandola, Forrest N and Shaw, Albert E and Krishna, Ravi and Keutzer, Kurt W},
	journal={arXiv preprint arXiv:2006.11316},
	year={2020}
}