@inproceedings{hacohen2019power,
	title={On the power of curriculum learning in training deep networks},
	author={Hacohen, Guy and Weinshall, Daphna},
	booktitle={International Conference on Machine Learning},
	pages={2535--2544},
	year={2019},
	organization={PMLR}
}
@article{narvekar2020curriculum,
	title={Curriculum learning for reinforcement learning domains: A framework and survey},
	author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={181},
	pages={1--50},
	year={2020}
}
@article{mermer2017scalable,
	title={Scalable Curriculum Learning for Artificial Neural Networks},
	author={Mermer, Melike Nur and Amasyali, Mehmet Fatih},
	journal={IPSI BGD TRANSACTIONS ON INTERNET RESEARCH},
	volume={13},
	number={2},
	year={2017},
	publisher={IPSI BELGRADE LTD DALMATINSKA 55, BELGRADE, 00000, SERBIA}
}
@article{wu2020curricula,
	title={When Do Curricula Work?},
	author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
	journal={arXiv preprint arXiv:2012.03107},
	year={2020}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={arXiv preprint arXiv:1706.03762},
	year={2017}
}
@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={arXiv preprint arXiv:2005.14165},
	year={2020}
}