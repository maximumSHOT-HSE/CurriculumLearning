% В этом шаблоне используется класс spbau-diploma. Его можно найти и, если требуется, 
% поправить в файле spbau-diploma.cls
\documentclass{spbau-diploma}
\usepackage{graphicx}
\begin{document}
% Год, город, название университета и факультета предопределены,
% но можно и поменять.
% Если англоязычная титульная страница не нужна, то ее можно просто удалить.
\filltitle{ru}{
    chair              = {\ },
    title              = {\textbf{ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА\\(БАКАЛАВРСКАЯ РАБОТА)}\vskip 0.5 em на тему \vskip 0.5 em\textbf{«Методы оценки сложности текстовых данных для ускорения обучения языковых моделей с помощью обучения по плану»}\\\textit{\small Направление подготовки 01.03.02 Прикладная математика и информатика}},
    % Здесь указывается тип работы. Возможные значения:
    %   coursework - Курсовая работа
    %   diploma - Диплом специалиста
    %   master - Диплом магистра
    %   bachelor - Диплом бакалавра
    % type               = {diploma},
    position           = {студента},
    group              = {БПМ171С},
    author             = {Сурков Максим Константинович},
    supervisorPosition = {?},
    supervisor         = {Ямщиков Иван Павлович},
    reviewerPosition   = {ст. преп.},
    reviewer           = {Привалов А.\,И.},
    % chairHeadPosition  = {д.\,ф.-м.\,н., профессор},
    % chairHead          = {Омельченко А.\,В.},
    university = {\textbf{Федеральное государственное автономное образовательное учреждение высшего образования\\«Национальный исследовательский университет\\«Высшая школа экономики»}},
    faculty = {Факультет Санкт-Петербургская школа физико-математических и компьютерных наук\\Департамент информатики},
    city = {Санкт-Петербург},
    year = {2021}
}
\maketitle

\tableofcontents
\section*{Аннотация}
\ 

Современные системы обработки естественного языка активно используют глубокие нейронные сети (BERT, GPT-3), которые требуют значительных ресурсов для их обучения. За последние годы было разработано множество подходов для решения данной проблемы. Одним из них является обучение по плану. Данный метод состоит из двух составляющих: оценки сложности тренировочных данных и алгоритма их семплирования. Основной целью данной работы является исследование метрик оценки сложности текстовых данных в контексте обучения по плану и влияния данного метода ускорения обучения на скорость сходимости языковых моделей на задачах предобучения и классификации. В процессе исследований было предложено и адаптировано несколько подходов из разных областей математики. Также были реализованы производительные алгоритмы вычисления найденных метрик на больших объемах данных в несколько десятков миллинов тренировочных примеров. Объемный набор экспериментов на задачах предобучения и классификации  выявил наиболее эффективные метрики для использования в обучении по плану. В то же время было установлено, что обучение по плану негативно влияет на скорость сходимости на задаче предобучения, однако не уступает базовому подходу (обучению без плана) на задаче классификации. Также был рассмотрен важный частный случай обучения языковой модели на шумном наборе тренировочных данных. Сравнительный анализ показал ускорение обучения до двух раз на первых 10\% обучения при применении обучения по плану с наиболее эффективной метрикой.
\\

Ключевые слова: обработка естественного языка, обучение по плану, теория информации, оценка сложности текстовых данных

\pagebreak

Modern state-of-the-art natural language processing systems use deep neural networks (BERT, GPT-3) that require many resources for training. Several techniques have been developed for the last ten years. One of them is curriculum learning, which consists of two parts, namely data complexity evaluation and sampling. The main purpose of this work is to research metrics of text complexity in the context of curriculum learning and explore the influence of curriculum learning on training time on pre-training and classification tasks. Several approaches from different mathematics fields were suggested and adapted during the research. Moreover, efficient algorithms for calculating given metrics on large datasets of several tens of millions of samples were implemented. Extensive experiments highlighted the most efficient metrics for use in curriculum learning. At the same time, it was established that curriculum learning negatively affects convergence time on pre-training task, but not inferior to the basic solution (learning without curriculum) on the classification task. Also, training on a noisy training dataset was considered. Comparative analysis showed a double reduction in training time on the first 10\% of training using curriculum learning with the most effective metric.

Keywords: natural language processing, curriculum learning, information theory, text complexity estimation
\section*{Введение}
\ 

На сегодняшний день существует множество сфер, где активно применяется обработка естественного языка. Например, в разработке голосовых помощников, алгоритмов фильтрации текста и машинного перевода. Возникающие задачи необходимо решать эффективно с точки зрения качества модели и скорости работы системы. За основу многих подходов взят механизм внимания~\cite{vaswani2017attention}. На его базе были разработаны модели, такие как BERT~\cite{devlin2018bert}, GPT-3~\cite{brown2020language} и многие другие. Данные сети имеют высокое качество, однако, за это приходится платить существенным временем обучения. В рамках данной работы исследуется влияние обучения по плану на примере тренировки модели BERT, так как она является одной из самых популярных моделей, имеет высокую точность и сравнительно небольшой размер для удобства постановки экспериментов. Также стоит отметить, что для обучения модели используют объемные корпуса данных, состоящие из нескольких десятков миллионов примеров, для которых нужны производительные алгоритмы их обработки. 

\begin{table}[h]
	\label{table:dataset_sizes}
	\caption{Количество примеров в тренировочных корпусах данных}
	\centering
	\begin{tabular}{|l|c|}
		\hline
		Корпус данных & Размер \\
		\hline\hline
		Wikipedia & 3-600M \\
		BookCorpus & 74M\\
		\hline\hline
		Hyperpartisan News Detection & 600k-2M \\
		sentiment140 & 1.6M \\
		IWSLT & 200-230k \\
		QQP & 364k \\
		MNLI & 393k \\
		\hline
	\end{tabular}
\end{table}

Процесс тренировки модели состоит из двух основных частей. Первая заключается в предобучении сети на задаче Masked Language Modelling~\cite{devlin2018bert}, которая состоит в восстановлении предложения после замены 15\% слов на пробелы. Предобучение занимает несколько недель исполнения кода на дорогостоящих графических процессорах. Второй этап представляет из себя задачу дообучения языковой модели, например на задачу классификации, и требует несколько дней даже на элементарных задачах, таких как определение спама или грубой речи~\cite{gertner2019mitre}. Одним из методов ускорения обучения моделей является обучение по плану~\cite{bengio2009curriculum}. При его применении данные сортируются по сложности, а затем семплируются с помощью заранее определенного алгоритма, который учитывает порядок данных. Данный подход хорошо себя показал во многих областях машинного обучения~\cite{narvekar2020curriculum, hacohen2019power, mermer2017scalable}, однако в обработке естественного языка существует лишь ограниченное число успешных работ~\cite{platanios2019competence, xu2020curriculum}. Более того, на данный момент нет исследований влияния обучения по плану на скорость сходимости модели на задачах предобучения и классификации. Также в существующих работах авторы уделяют большое внимание алгоритмам семплирования данных, а метрики берут из достаточного узкого множества, которое можно значительно расширить, применив различные сферы компьютерных наук. Это позволяет обозначить широкое поле для исследований подходов к оценке сложности текстов и предположить, что существует метрика, которая позволит значительно ускорить обучение модели на вышеуказанных задачах. Однако, в последнее время стали появляться работы с отрицательными результатами применения обучения по плану на задачах компьютерного зрения~\cite{wu2020curricula}, что показывает спорную репутацию данного подхода к ускорению. В то же время, в тех же статьях авторы находят частные случаи применения обучения по плану на практике. Таким образом, можно выделить обширную сферу исследований нетривиального вопроса применимости обучения по плану к ускорению тренировочного процесса языковых моделей на задачах предобучения и классификации.

\subsection*{Цель и задачи}
\ 

Целью данной работы является ускорение обучения языковой модели BERT c помощью обучения по плану за счет применения улучшенной метрики сложности текстовых данных на задачах классификации и предобучения
\begin{itemize}
	\item Предложить метрики оценки сложности текста
	\item Реализовать производительные алгоритмы вычисления предложенных метрик на больших корпусах данных
	\item Сравнить найденные метрики
	\item Исследовать влияние найденных метрик на скорость обучения языковой модели BERT на чистых и шумных тренировочных данных
\end{itemize}
\subsection*{Достигнутые результаты}
\subsection*{Структура работы}
\section{Обзор литературы}
\subsection{Возникновение обучения по плану}
\ 

Точная дата возникновения обучения по плану не известна, но можно выделить логическое начало в работе Bengio 2009 года~\cite{bengio2009curriculum}, в которой было показано, что обучение по плану может привести к улучшению качества моделей машинного обучения. Авторы поставили несколько экспериментов, одним из которых является опыт по обучению классификатора геометрических фигур. Было обнаружено, что если сначала предъявить модели более простые примеры (квадраты, круги, равнобедренные треугольники) перед стандартным алгоритмом обучения, то итоговое качество возрастет. Этот простой пример подчеркивает большой потенциал обучения по плану к улучшению существующих алгоритмов в машинном обучении.
\subsection{Применение обучения по плану в смежных сферах машинного обучения}
\ 

Обучение по плану активно применялось в разных областях машинного обучения в течение последних нескольких лет. Например, Hacohen и Weinshall в 2019 году~\cite{hacohen2019power} применили данный метод к задачам компьютерного зрения. Они предложили модельную метрику оценки сложности картинок, которая считается следующим образом. Рассматривается независимая модель, предобученная на датасете ImageNet. Далее сложность примера определяется как уверенность модели в своем предсказании. Наконец, ученые использовали лестничный алгоритм семплирования в паре с предложенной метрикой. В результате был получен прирост в скорости обучения и в качестве итоговой модели.

Narvekar и др.~\cite{narvekar2020curriculum} применили обучение по плану в обучении с подкреплением следующим образом. ({\bf ДОДЕЛАТЬ})

Обуение по плану применяется в и классическом глубоком обучении. Mermer и др.~\cite{mermer2017scalable} предлагают следующее использование. ({\bf ДОДЕЛАТЬ})

Важно отметить, что при применении обучения по плану можно получить и отрицательный результат. Так Wu и др.~\cite{wu2020curricula} выявили негативное влияние обучения по плану на скорость обучения широкого множества глубоких нейронный сетей на задаче классификации картинок. Авторы применили данный подход к более чем сотне архитектур, среди которых ResNet и VGG-19. Авторы рассмотрели несколько метрик сложности данных, которые сильно коррелировали с величиной $s(x_i, y_i)$ последней эпохи $t$	, после которой модель $w$ правильно отвечает на пример $(x_i, y_i)$ вплоть до последней эпохи $T$ (предсказание модели $\hat{y}_w(t)$ совпадает с реальной меткой $y_i$ примера), для которого считается сложность (формула~(\ref{eq:learned_epoch})). 

\begin{equation} \label{eq:learned_epoch}
s(x_i, y_i) = \min_{t^*}\{\hat{y}_w(t)_i = y_i,\forall t^* \le t \le T\}
\end{equation}

Было использовано семейство "префиксных" семплеров (рис.~\ref{fig:cv_pacing_functions}), а именно возрастающих функций, которые определяют долю легких примеров $g(t)$ в конкретный момент обучения $t$. Таким образом, данные семлперы в момент времени $t$ строят батч данных, равноверятно выбираея примеры из первых $g(t)$ семплов отсортированного по метрике сложности датасета. В результате, ученые показали, что при использовании данной конфигурации обучения по плану не приводит ни к улучшению качества итоговой модели, ни к ускорению обучения. Также было рассмотрено два важных частных случая, а именно обучение на шумных данных и обучение с ограниченным числом тренировочных шагов. При добавлении 20\% шума в тренировочный корпус, обучение по плану позволяет улучшить точность модели на 10\%. Влияние же подхода на скорость обучения исследовано не было.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.48]{CV_pacing_functions}
	\caption{Семейство функций для семплирования данных}
	\label{fig:cv_pacing_functions}
\end{figure}

\subsection{Применение обучения по плану в обработке естественного языка}
\ 

В обработке естественного языка существует ограниченное число существенных результатов в контексте применения обучения по плану. Вероятно, это связано с тем, что естественный язык состоит из слов и предложений, не имеющих строгую структуру, которую можно формально описать. Более того, на данный момент наука не понимает всех процессов, происходящих внутри современных языковых моделей.

Platanios и др.~\cite{platanios2019competence} исследовали влияние обучения по плану на задаче машинного перевода на скосрость сходимости нейронных сетей. Авторы рассмотрели две метрики сложности текстов: длину и вероятность правдоподобия, которая вычисляется по формуле (\ref{eq:log_likelihood}), где $s_i$ -- текст или набор токенов, $w_k^i$ -- слово или токен, $p(x)$ -- доля токенов $x$ во всем датасете.

\begin{equation} \label{eq:log_likelihood}
d(s_i) = -\sum\limits_{k=1}^{N}\log p(w_k^i)
\end{equation}

Они показали, что правдоподобие не имеет никаких преимуществ по сравнению  с длиной с точки зрения скорости обучения моделей. В качестве семплеров был взят префиксный семплер с функцией $c(t)$ (формула (\ref{eq:competence_based_sampler})), вычисляющий долю простых примеров, доступных для построения батча.

\begin{equation} \label{eq:competence_based_sampler}
c(t) = \min\left(1, \sqrt{t\frac{1 - c_0^2}{T} + c_0^2}\right)
\end{equation}

$T$ -- общее число тренировочных шагов, $c_0$ -- доля простых примеров, доступных в самом начале обучения (авторы используют $c_0 = 0.01$)

В итоге, ученые добились улучшения качества модели на 2.2 BLEU и ускорения обучения на 70\%.

Xu и др.~\cite{xu2020curriculum} предложили альтернативный способ применения обучения по плану в обработке естественного языка на задаче Natural Language Understanding. Их метод оперирует понятием модельной оценки сложности текстов, что приводит к тому, что сложность примеров меняется в процессе обучения в зависимости от качества модели на момент применения метрики к примеру. Авторы предлагают алгоритм, который в цикле повторяет следующую процедуру. Тренировочный корпус данных разделяется на несколько частей. Затем, для каждого блока независимо обучается новая модель, которая инициализируется весами текущей глобальной модели. После этого оценивается сложность всех примеров как сумма уверенностей всех моделей по всем блокам кроме блока, в котором находится данный пример (рис. \ref{fig:acl20_algo_difficulty}).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.48]{acl20_algo_difficulty}
	\caption{Алгоритм вычисления модельной метрики сложности текста. На данной схеме датасет делится на $4$ части, на каждой из которых учится независимая модель BERT с началными весами текущей глобальной модели. Сложность примеров в блоке под номером $3$ вычисляется как сумма уверенностей моделей $W_1,W_2,W_4$.}
	\label{fig:acl20_algo_difficulty}
\end{figure}

Наконец, весь датасет сортируется в соответствии с найденными сложностями примеров, и текущая глобальная модель обучается на новой эпохе, обрабатывая примеры в порядке возрастания сложности. Данный подход позволяет улучшить точность итогвоой модели на 1.5\%, но требует существенно больше времени.

({\bf ДОБАВИТЬ ОПИСАНИЕ Kocmi, Xuan Zhang})
\subsection{Существующие метрики оценки сложности текстов}
\subsection{Выводы}
\section{Глава1}
\section{Глава2}
\section{Глава3}
\section{Глава4}
\section{Заключение}

% размещенный с предпочтением "вверху страницы"
% \begin{figure}[t]
% \centering
% \includegraphics{fig1.jpg}
% \caption{Разрыв функции}
% \label{разрыв_функции}
% \end{figure}

% \begin{figure}[h]
    % \includegraphics{thesis-search-trends}
    % \caption{Статистика поисковых запросов в течении года}
% \end{figur

\bibliographystyle{ugost2008ls}
\bibliography{diploma.bib}
\end{document}
